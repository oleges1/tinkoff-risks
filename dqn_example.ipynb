{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from collections import deque\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, InputLayer\n",
    "from keras.models import Model\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем собрать DQN для простой среды - перевернутого маятника"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так она выглядит:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i.ytimg.com/vi/5SEEwqRH8_c/hqdefault.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так выглядят случайные действия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "sleep(0.1)\n",
    "done = False\n",
    "while done != True:\n",
    "    a = env.action_space.sample()\n",
    "    s2, r, done, info = env.step(a)\n",
    "    env.render()\n",
    "    sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install pyglet==1.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем агента, которого мы будем обучать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, hidden_dims=[32, 32], lr=0.01):\n",
    "        '''\n",
    "        Для инициализации агента передаем среду из gym,\n",
    "        размеры скрытых слоев сети hidden_dims, \n",
    "        и скорость обучения lr\n",
    "        '''\n",
    "        self.env = env\n",
    "        self.input_dim = env.observation_space.shape[0]\n",
    "        self.output_dim = env.action_space.n\n",
    "        self.create_model(hidden_dims, lr)\n",
    "\n",
    "    def create_model(self, hidden_dims, lr):\n",
    "        '''\n",
    "        Описываем полносвязную нейронную сеть, у которой количесвто входов - \n",
    "        размерность состояния, количество выходов - количество действий.\n",
    "        Количесвтво внутренних скрытых слоев передаем в hidden_dims.\n",
    "        '''\n",
    "        self.model = Sequential()\n",
    "        self.model.add(InputLayer(input_shape=(self.input_dim,)))\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.model.add(Dense(units=hidden_dim, activation='relu'))\n",
    "        self.model.add(Dense(units=self.output_dim))\n",
    "        self.model.compile(loss=keras.losses.mse,\n",
    "                           optimizer=keras.optimizers.SGD(lr))\n",
    "\n",
    "    def act(self, X, eps=1.0):\n",
    "        '''\n",
    "        Выбираем действие для состояния X.\n",
    "        Действуем эпсилон-жадно с заданным eps.\n",
    "        '''\n",
    "        if np.random.rand() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        X = X.reshape(1, self.input_dim)\n",
    "        Q = self.model.predict_on_batch(X)\n",
    "        return np.argmax(Q, 1)[0]\n",
    "\n",
    "    def train(self, X_batch, y_batch):\n",
    "        '''\n",
    "        Делаем шаг обучения для батча состояний X_batch и таргетов y_batch.\n",
    "        '''\n",
    "        return self.model.train_on_batch(X_batch, y_batch)\n",
    "\n",
    "    def predict(self, X_batch):\n",
    "        '''\n",
    "        Делаем предсказания Q-значений для батча состояний X_batch.\n",
    "        '''\n",
    "        return self.model.predict_on_batch(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_batch(agent, memory, batch_size, gamma):\n",
    "    '''\n",
    "    Функция собирает батч размера batch_size из памяти memory\n",
    "    для агента agent c коэффициентом дисконтирования gamma.\n",
    "    Для сделанного действия таргет считаем по алгоритму Q-обучения, \n",
    "    для остальных берем предсказание агента.\n",
    "    Функция возвращает батч состояний X_batch и батч таргетов y_batch.\n",
    "    '''\n",
    "    sample = random.sample(memory, batch_size)\n",
    "    sample = np.asarray(sample)\n",
    "\n",
    "    s = sample[:, 0]\n",
    "    a = sample[:, 1].astype(np.int8)\n",
    "    r = sample[:, 2]\n",
    "    s2 = sample[:, 3]\n",
    "    d = sample[:, 4] * 1.\n",
    "\n",
    "    X_batch = np.vstack(s)\n",
    "    y_batch = agent.predict(X_batch)\n",
    "\n",
    "    y_batch[np.arange(batch_size), a] = r + gamma * np.max(agent.predict(np.vstack(s2)), 1) * (1 - d)\n",
    "\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_agent_play(env_name, agent, eps):\n",
    "    '''\n",
    "    Визуализация поведения в gym среде с названием env_name \n",
    "    агента agent по эпсилон-жадной стратегии с заданным eps.\n",
    "    Функция возвращает сумарный reward показанного эпизода.\n",
    "    '''\n",
    "    env = gym.make(env_name)\n",
    "    s = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while done != True:\n",
    "        a = agent.act(s, eps)\n",
    "        s2, r, done, info = env.step(a)\n",
    "        episode_reward += r\n",
    "        env.render()\n",
    "        s = s2\n",
    "        sleep(0.01)\n",
    "    env.close()\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent_play('CartPole-v0', agent, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим описанного агента для выбранной среды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}\n",
      "Game solved in {episode + 1} steps with average reward {np.mean(LAST_30_GAME_EPISODE_REWARDS)}\n"
     ]
    }
   ],
   "source": [
    "n_episode = 1000\n",
    "gamma = 0.9\n",
    "n_memory = 50000\n",
    "batch_size = 100\n",
    "eps = 0.5\n",
    "eps_decay = 0.98\n",
    "min_eps = 0.05\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "agent = Agent(env)\n",
    "memory = deque()\n",
    "# Условием окончания игры будет определенное среденее значение награды за 30 эпизодов\n",
    "LAST_30_GAME_EPISODE_REWARDS = deque()\n",
    "target_reward = 195.0\n",
    "\n",
    "for episode in range(n_episode):\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        a = agent.act(s, eps)\n",
    "        s2, r, done, info = env.step(a)\n",
    "        episode_reward += r\n",
    "\n",
    "        memory.append([s, a, r, s2, done])\n",
    "\n",
    "        if len(memory) > n_memory:\n",
    "            memory.popleft()\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            X_batch, y_batch = create_batch(agent, memory, batch_size, gamma)\n",
    "            agent.train(X_batch, y_batch)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "    print('[Episode {episode:>5}] Reward: {episode_reward:>5} EPS: {eps:>3.2f}')\n",
    "    \n",
    "    LAST_30_GAME_EPISODE_REWARDS.append(episode_reward)\n",
    "    if len(LAST_30_GAME_EPISODE_REWARDS) > 30:\n",
    "        LAST_30_GAME_EPISODE_REWARDS.popleft()\n",
    "\n",
    "    if np.mean(LAST_30_GAME_EPISODE_REWARDS) >= target_reward:\n",
    "        print(\"Game solved in {episode + 1} steps with average reward {np.mean(LAST_30_GAME_EPISODE_REWARDS)}\")\n",
    "        break\n",
    "    \n",
    "    eps = max(min_eps, eps*eps_decay)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как действует обученный агент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent_play(env_name, agent, eps=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent_play(env_name, agent, eps=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "247.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent_play('CartPole-v1', agent, eps=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что как только среда расширяется - наш агент не может адекватно с ней дружить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что тут еще можно попробовать, что должно работать на любой системе:\n",
    "* обучиться в CartPole-v1 до скора 495\n",
    "* попробовать поиграть в [Acrobot-v1](https://gym.openai.com/envs/Acrobot-v1/) до скора -120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хочется нырнуть поглубже, то можно попрбовать посадить [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/) или поиграть в [Atari](https://gym.openai.com/envs/#atari) - там для каждой из игр есть версия где нужно учиться по картинке и версия где нужно учиться по реестровым численным данным приставки. Но для этих сред понадобятся библиотеки box2d-py и atari-py, с которыми все будет хорошо под unixовыми системами, а вот под виндой придется плясать с бубном."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
